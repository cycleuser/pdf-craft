#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è¿›çš„PDF OCRå¯åŠ¨è„šæœ¬
ç»“åˆäº†åŸºç¡€åŠŸèƒ½æ£€æŸ¥å’Œæ€§èƒ½ä¼˜åŒ–é…ç½®
æ”¯æŒå¤–ç½‘è®¿é—®å’Œå®Œæ•´çš„ç³»ç»Ÿæ£€æµ‹
"""

import os
import sys
import platform
import logging
import argparse
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from app import app
from optimization_config import OptimizationConfigManager, auto_configure_optimization, get_preset_config, list_preset_configs

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def print_banner():
    """æ‰“å°å¯åŠ¨æ¨ªå¹…"""
    banner = """
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                PDF OCR æ™ºèƒ½å¤„ç†å¹³å°                           â•‘
    â•‘                                                              â•‘
    â•‘  ğŸš€ è‡ªåŠ¨æ£€æµ‹ç¡¬ä»¶é…ç½®å¹¶åº”ç”¨æœ€ä¼˜è®¾ç½®                            â•‘
    â•‘  âš¡ æœ€å¤§åŒ–åˆ©ç”¨GPUã€CPUå’Œå†…å­˜èµ„æº                             â•‘
    â•‘  ğŸ“ˆ æ˜¾è‘—æå‡PDF OCRå¤„ç†é€Ÿåº¦                                  â•‘
    â•‘  ğŸŒ æ”¯æŒå¤–ç½‘è®¿é—®å’Œæœ¬åœ°éƒ¨ç½²                                    â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    print(banner)

def check_gpu_availability():
    """æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨"""
    try:
        import torch
        cuda_available = torch.cuda.is_available()
        if cuda_available:
            device_count = torch.cuda.device_count()
            device_name = torch.cuda.get_device_name(0) if device_count > 0 else "Unknown"
            memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3 if device_count > 0 else 0
            return True, f"å¯ç”¨ ({device_count} ä¸ªè®¾å¤‡, {device_name}, {memory_gb:.1f}GB)"
        else:
            return False, "ä¸å¯ç”¨ (CUDAæœªå¯ç”¨)"
    except ImportError:
        return False, "ä¸å¯ç”¨ (PyTorchæœªå®‰è£…)"
    except Exception as e:
        return False, f"ä¸å¯ç”¨ (é”™è¯¯: {str(e)})"

def check_models():
    """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶"""
    model_dir = app.config['MODEL_DIR']
    if not os.path.exists(model_dir):
        os.makedirs(model_dir, exist_ok=True)
        return f"æ¨¡å‹ç›®å½•å·²åˆ›å»º: {model_dir}ï¼Œé¦–æ¬¡è¿è¡Œæ—¶å°†è‡ªåŠ¨ä¸‹è½½æ¨¡å‹"

    model_files = os.listdir(model_dir)
    if not model_files:
        return f"æ¨¡å‹ç›®å½•ä¸ºç©º: {model_dir}ï¼Œé¦–æ¬¡è¿è¡Œæ—¶å°†è‡ªåŠ¨ä¸‹è½½æ¨¡å‹"

    # æ£€æŸ¥å…³é”®æ¨¡å‹æ–‡ä»¶
    key_models = {
        "DocLayout-YOLO": ["doclayout_yolo.pt", "yolov8n.pt"],
        "OnnxOCR": ["model_cn.onnx", "model_en.onnx"],
        "LayoutReader": ["layoutreader.onnx"]
    }

    model_status = {}
    for model_name, files in key_models.items():
        found = []
        for root, _, filenames in os.walk(model_dir):
            for filename in filenames:
                if any(filename.endswith(f) or f in filename for f in files):
                    found.append(filename)
        model_status[model_name] = f"{'âœ“' if found else 'âœ—'} ({', '.join(found[:2]) if found else 'æœªæ‰¾åˆ°'})"

    return model_status

def check_onnxruntime():
    """æ£€æŸ¥onnxruntimeç‰ˆæœ¬"""
    try:
        import onnxruntime
        version = onnxruntime.__version__
        providers = onnxruntime.get_available_providers()
        gpu_support = "CUDAExecutionProvider" in providers
        return f"ç‰ˆæœ¬ {version}, GPUæ”¯æŒ: {'æ˜¯' if gpu_support else 'å¦'}"
    except ImportError:
        return "æœªå®‰è£…"
    except Exception as e:
        return f"é”™è¯¯: {str(e)}"

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–é¡¹"""
    logger.info("æ£€æŸ¥ä¾èµ–é¡¹...")
    
    missing_deps = []
    
    # æ£€æŸ¥å¿…éœ€çš„åŒ…
    required_packages = [
        ('torch', 'PyTorch'),
        ('pdf_craft', 'PDF-Craft'),
        ('flask', 'Flask'),
        ('psutil', 'psutil'),
        ('fitz', 'PyMuPDF')
    ]
    
    for package, name in required_packages:
        try:
            __import__(package)
            logger.info(f"âœ… {name} å·²å®‰è£…")
        except ImportError:
            missing_deps.append(name)
            logger.warning(f"âŒ {name} æœªå®‰è£…")
    
    if missing_deps:
        print(f"\nâš ï¸  ç¼ºå°‘ä¾èµ–é¡¹: {', '.join(missing_deps)}")
        print("è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…:")
        print("pip install -r requirements.txt")
        return False
    
    return True

def setup_environment():
    """è®¾ç½®ç¯å¢ƒå˜é‡"""
    logger.info("è®¾ç½®ç¯å¢ƒå˜é‡...")
    
    # è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡
    os.environ.setdefault('CUDA_VISIBLE_DEVICES', '0')
    
    # è®¾ç½®å†…å­˜å¢é•¿ç­–ç•¥
    os.environ.setdefault('TF_FORCE_GPU_ALLOW_GROWTH', 'true')
    
    # è®¾ç½®çº¿ç¨‹æ•°
    cpu_count = os.cpu_count() or 1
    os.environ.setdefault('OMP_NUM_THREADS', str(min(8, cpu_count)))
    os.environ.setdefault('MKL_NUM_THREADS', str(min(8, cpu_count)))
    
    logger.info("ç¯å¢ƒå˜é‡è®¾ç½®å®Œæˆ")

def apply_optimization_config(preset_name=None):
    """åº”ç”¨ä¼˜åŒ–é…ç½®"""
    print("\nğŸ”§ åº”ç”¨æ€§èƒ½ä¼˜åŒ–é…ç½®...")
    
    try:
        if preset_name:
            # ä½¿ç”¨é¢„è®¾é…ç½®
            preset = get_preset_config(preset_name)
            print(f"ğŸ›ï¸  åº”ç”¨é¢„è®¾é…ç½®: {preset['name']}")
            print(f"   æè¿°: {preset['description']}")
            
            config = preset['config']
            for key, value in config.items():
                config_key = key.upper()
                if hasattr(app.config, config_key):
                    app.config[config_key] = value
                    
            return True, f"å·²åº”ç”¨é¢„è®¾é…ç½®: {preset['name']}"
        else:
            # è‡ªåŠ¨æ£€æµ‹å’Œé…ç½®
            optimal_config, recommendations = auto_configure_optimization()
            
            # åº”ç”¨ä¼˜åŒ–é…ç½®
            app.config['USE_GPU'] = optimal_config.get('device') == 'cuda'
            app.config['ENABLE_MULTIPROCESSING'] = optimal_config.get('enable_multiprocessing', False)
            app.config['ENABLE_MIXED_PRECISION'] = optimal_config.get('enable_mixed_precision', False)
            app.config['OPTIMIZE_MEMORY'] = optimal_config.get('optimize_memory', True)
            app.config['PRELOAD_MODELS'] = optimal_config.get('preload_models', False)
            app.config['PROCESS_POOL_SIZE'] = optimal_config.get('process_pool_size', 2)
            app.config['GPU_BATCH_SIZE'] = optimal_config.get('gpu_batch_size', 4)
            app.config['CPU_BATCH_SIZE'] = optimal_config.get('cpu_batch_size', 2)
            app.config['MAX_WORKERS'] = optimal_config.get('max_workers', 4)
            
            print(f"âœ… GPUåŠ é€Ÿ: {'å¯ç”¨' if app.config['USE_GPU'] else 'ç¦ç”¨'}")
            print(f"âœ… å¤šè¿›ç¨‹å¤„ç†: {'å¯ç”¨' if app.config['ENABLE_MULTIPROCESSING'] else 'ç¦ç”¨'}")
            print(f"âœ… æ··åˆç²¾åº¦: {'å¯ç”¨' if app.config['ENABLE_MIXED_PRECISION'] else 'ç¦ç”¨'}")
            print(f"âœ… å†…å­˜ä¼˜åŒ–: {'å¯ç”¨' if app.config['OPTIMIZE_MEMORY'] else 'ç¦ç”¨'}")
            print(f"âœ… æ¨¡å‹é¢„åŠ è½½: {'å¯ç”¨' if app.config['PRELOAD_MODELS'] else 'ç¦ç”¨'}")
            print(f"ğŸ“Š æ€§èƒ½ç­‰çº§: {recommendations['performance_tier']}")
            print(f"ğŸ“ˆ é¢„æœŸæå‡: {recommendations['estimated_speedup']}")
            
            return True, f"è‡ªåŠ¨ä¼˜åŒ–å®Œæˆï¼Œæ€§èƒ½ç­‰çº§: {recommendations['performance_tier']}"
            
    except Exception as e:
        logger.error(f"ä¼˜åŒ–é…ç½®åº”ç”¨å¤±è´¥: {str(e)}")
        return False, f"ä¼˜åŒ–å¤±è´¥: {str(e)}"

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='PDF OCR æ™ºèƒ½å¤„ç†å¹³å°')
    parser.add_argument('--host', default='0.0.0.0', help='æœåŠ¡å™¨åœ°å€ (é»˜è®¤: 0.0.0.0ï¼Œæ”¯æŒå¤–ç½‘è®¿é—®)')
    parser.add_argument('--port', type=int, default=5000, help='æœåŠ¡å™¨ç«¯å£ (é»˜è®¤: 5000)')
    parser.add_argument('--debug', action='store_true', help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
    parser.add_argument('--no-auto-config', action='store_true', help='ç¦ç”¨è‡ªåŠ¨ä¼˜åŒ–é…ç½®')
    parser.add_argument('--preset', choices=['maximum_speed', 'balanced', 'maximum_quality', 'low_resource'], 
                       help='ä½¿ç”¨é¢„è®¾é…ç½®')
    parser.add_argument('--list-presets', action='store_true', help='åˆ—å‡ºæ‰€æœ‰é¢„è®¾é…ç½®')
    
    args = parser.parse_args()
    
    # åˆ—å‡ºé¢„è®¾é…ç½®
    if args.list_presets:
        presets = list_preset_configs()
        print("\nå¯ç”¨çš„é¢„è®¾é…ç½®:")
        for name, info in presets.items():
            print(f"  {name}: {info['name']} - {info['description']}")
        return
    
    # æ‰“å°å¯åŠ¨æ¨ªå¹…
    print_banner()
    
    # æ£€æŸ¥ä¾èµ–é¡¹
    if not check_dependencies():
        sys.exit(1)
    
    # è®¾ç½®ç¯å¢ƒ
    setup_environment()
    
    # ç¡®ä¿æ‰€æœ‰å¿…éœ€ç›®å½•å­˜åœ¨
    os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
    os.makedirs(app.config['RESULTS_FOLDER'], exist_ok=True)
    os.makedirs(app.config['MODEL_DIR'], exist_ok=True)
    
    # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    print(f"\nğŸ“Š ç³»ç»Ÿä¿¡æ¯:")
    print(f"  æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()} ({platform.architecture()[0]})")
    print(f"  Pythonç‰ˆæœ¬: {platform.python_version()}")
    
    # æ£€æŸ¥GPUå¯ç”¨æ€§
    gpu_available, gpu_info = check_gpu_availability()
    print(f"  GPUçŠ¶æ€: {gpu_info}")
    
    # æ£€æŸ¥ONNX Runtime
    onnx_info = check_onnxruntime()
    print(f"  ONNX Runtime: {onnx_info}")
    
    # æ£€æŸ¥æ¨¡å‹
    model_status = check_models()
    if isinstance(model_status, dict):
        print(f"\nğŸ“ æ¨¡å‹çŠ¶æ€:")
        for model_name, status in model_status.items():
            print(f"  {model_name}: {status}")
    else:
        print(f"\nğŸ“ æ¨¡å‹çŠ¶æ€: {model_status}")
    
    print(f"\nğŸ“‚ ç›®å½•é…ç½®:")
    print(f"  æ¨¡å‹ç›®å½•: {app.config['MODEL_DIR']}")
    print(f"  ä¸Šä¼ ç›®å½•: {app.config['UPLOAD_FOLDER']}")
    print(f"  ç»“æœç›®å½•: {app.config['RESULTS_FOLDER']}")
    
    # åº”ç”¨ä¼˜åŒ–é…ç½®
    if not args.no_auto_config:
        success, message = apply_optimization_config(args.preset)
        if success:
            print(f"âœ… {message}")
        else:
            print(f"âš ï¸  {message}")
            print("å°†ä½¿ç”¨é»˜è®¤é…ç½®å¯åŠ¨...")
    else:
        print("âš ï¸  å·²ç¦ç”¨è‡ªåŠ¨ä¼˜åŒ–é…ç½®")
    
    # å¯åŠ¨åº”ç”¨
    print(f"\nğŸš€ å¯åŠ¨OCRæœåŠ¡...")
    print(f"   æœ¬åœ°è®¿é—®: http://localhost:{args.port}")
    if args.host == '0.0.0.0':
        print(f"   å¤–ç½‘è®¿é—®: http://[æ‚¨çš„IPåœ°å€]:{args.port}")
    print(f"   è°ƒè¯•æ¨¡å¼: {'å¼€å¯' if args.debug else 'å…³é—­'}")
    print("\næŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
    print("=" * 60)
    
    try:
        app.run(
            host=args.host,
            port=args.port,
            debug=args.debug,
            threaded=True
        )
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ æœåŠ¡å·²åœæ­¢")
        logger.info("åº”ç”¨æ­£å¸¸é€€å‡º")
    except Exception as e:
        logger.error(f"åº”ç”¨å¯åŠ¨å¤±è´¥: {str(e)}")
        print(f"\nâŒ å¯åŠ¨å¤±è´¥: {str(e)}")
        sys.exit(1)

if __name__ == '__main__':
    main() 