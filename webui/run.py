#!/usr/bin/env python
"""
Run script for the PDF to Markdown Web Converter
"""

import os
import sys
import platform
import logging
import subprocess
from pathlib import Path
from app import app
from optimization_config import OptimizationConfigManager, auto_configure_optimization

def check_gpu_availability():
    """æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨"""
    try:
        import torch
        cuda_available = torch.cuda.is_available()
        if cuda_available:
            device_count = torch.cuda.device_count()
            device_name = torch.cuda.get_device_name(0) if device_count > 0 else "Unknown"
            return True, f"å¯ç”¨ ({device_count} ä¸ªè®¾å¤‡, {device_name})"
        else:
            return False, "ä¸å¯ç”¨ (CUDAæœªå¯ç”¨)"
    except ImportError:
        return False, "ä¸å¯ç”¨ (PyTorchæœªå®‰è£…)"
    except Exception as e:
        return False, f"ä¸å¯ç”¨ (é”™è¯¯: {str(e)})"

def check_models():
    """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶"""
    model_dir = app.config['MODEL_DIR']
    if not os.path.exists(model_dir):
        os.makedirs(model_dir, exist_ok=True)
        return f"æ¨¡å‹ç›®å½•å·²åˆ›å»º: {model_dir}ï¼Œé¦–æ¬¡è¿è¡Œæ—¶å°†è‡ªåŠ¨ä¸‹è½½æ¨¡å‹"

    model_files = os.listdir(model_dir)
    if not model_files:
        return f"æ¨¡å‹ç›®å½•ä¸ºç©º: {model_dir}ï¼Œé¦–æ¬¡è¿è¡Œæ—¶å°†è‡ªåŠ¨ä¸‹è½½æ¨¡å‹"

    # æ£€æŸ¥å…³é”®æ¨¡å‹æ–‡ä»¶
    key_models = {
        "DocLayout-YOLO": ["doclayout_yolo.pt", "yolov8n.pt"],
        "OnnxOCR": ["model_cn.onnx", "model_en.onnx"],
        "LayoutReader": ["layoutreader.onnx"]
    }

    model_status = {}
    for model_name, files in key_models.items():
        found = [f for f in files if any(os.path.join(root, f).endswith(f) for root, _, files in os.walk(model_dir) for f in files)]
        model_status[model_name] = f"{'âœ“' if found else 'âœ—'} ({', '.join(found) if found else 'æœªæ‰¾åˆ°'})"

    return model_status

def check_onnxruntime():
    """æ£€æŸ¥onnxruntimeç‰ˆæœ¬"""
    try:
        import onnxruntime
        version = onnxruntime.__version__
        providers = onnxruntime.get_available_providers()
        gpu_support = "CUDAExecutionProvider" in providers
        return f"ç‰ˆæœ¬ {version}, GPUæ”¯æŒ: {'æ˜¯' if gpu_support else 'å¦'}, å¯ç”¨æä¾›è€…: {', '.join(providers)}"
    except ImportError:
        return "æœªå®‰è£…"
    except Exception as e:
        return f"é”™è¯¯: {str(e)}"

if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    logger = logging.getLogger("pdf-markdown-converter")

    # Ensure all required directories exist
    os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
    os.makedirs(app.config['RESULTS_FOLDER'], exist_ok=True)
    os.makedirs(app.config['MODEL_DIR'], exist_ok=True)

    # Get port from command line arguments or use default
    port = int(sys.argv[1]) if len(sys.argv) > 1 else 5000

    # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    print("\n===== PDF to Markdown Web Converter =====")
    print(f"ç³»ç»Ÿä¿¡æ¯: {platform.system()} {platform.release()} ({platform.architecture()[0]})")
    print(f"Pythonç‰ˆæœ¬: {platform.python_version()}")

    # æ£€æŸ¥GPUå¯ç”¨æ€§
    gpu_available, gpu_info = check_gpu_availability()
    print(f"GPUåŠ é€Ÿ: {gpu_info}")

    # æ£€æŸ¥ONNX Runtime
    onnx_info = check_onnxruntime()
    print(f"ONNX Runtime: {onnx_info}")

    # åº”ç”¨ä¼˜åŒ–é…ç½®
    print("\nğŸ”§ åº”ç”¨æ€§èƒ½ä¼˜åŒ–é…ç½®...")
    try:
        optimal_config, recommendations = auto_configure_optimization()
        
        # åº”ç”¨ä¼˜åŒ–é…ç½®
        app.config['USE_GPU'] = optimal_config.get('device') == 'cuda'
        app.config['ENABLE_MULTIPROCESSING'] = optimal_config.get('enable_multiprocessing', False)
        app.config['ENABLE_MIXED_PRECISION'] = optimal_config.get('enable_mixed_precision', False)
        app.config['OPTIMIZE_MEMORY'] = optimal_config.get('optimize_memory', True)
        app.config['PRELOAD_MODELS'] = optimal_config.get('preload_models', False)
        app.config['PROCESS_POOL_SIZE'] = optimal_config.get('process_pool_size', 2)
        app.config['GPU_BATCH_SIZE'] = optimal_config.get('gpu_batch_size', 4)
        app.config['CPU_BATCH_SIZE'] = optimal_config.get('cpu_batch_size', 2)
        app.config['MAX_WORKERS'] = optimal_config.get('max_workers', 4)
        
        print(f"âœ… GPUåŠ é€Ÿ: {'å¯ç”¨' if app.config['USE_GPU'] else 'ç¦ç”¨'}")
        print(f"âœ… å¤šè¿›ç¨‹å¤„ç†: {'å¯ç”¨' if app.config['ENABLE_MULTIPROCESSING'] else 'ç¦ç”¨'}")
        print(f"âœ… æ··åˆç²¾åº¦: {'å¯ç”¨' if app.config['ENABLE_MIXED_PRECISION'] else 'ç¦ç”¨'}")
        print(f"âœ… å†…å­˜ä¼˜åŒ–: {'å¯ç”¨' if app.config['OPTIMIZE_MEMORY'] else 'ç¦ç”¨'}")
        print(f"âœ… æ¨¡å‹é¢„åŠ è½½: {'å¯ç”¨' if app.config['PRELOAD_MODELS'] else 'ç¦ç”¨'}")
        print(f"ğŸ“Š æ€§èƒ½ç­‰çº§: {recommendations['performance_tier']}")
        print(f"ğŸ“ˆ é¢„æœŸæå‡: {recommendations['estimated_speedup']}")
        
    except Exception as e:
        logger.warning(f"ä¼˜åŒ–é…ç½®åº”ç”¨å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {str(e)}")
        # è®¾ç½®é»˜è®¤è®¾å¤‡
        if gpu_available:
            app.config['USE_GPU'] = True
            print("å·²è‡ªåŠ¨å¯ç”¨GPUåŠ é€Ÿ")
        else:
            app.config['USE_GPU'] = False
            print("ä½¿ç”¨CPUè¿›è¡Œå¤„ç†")

    # æ£€æŸ¥æ¨¡å‹
    model_status = check_models()
    if isinstance(model_status, dict):
        print("\næ¨¡å‹çŠ¶æ€:")
        for model_name, status in model_status.items():
            print(f"  - {model_name}: {status}")
    else:
        print(f"\næ¨¡å‹çŠ¶æ€: {model_status}")

    print(f"\næ¨¡å‹ç›®å½•: {app.config['MODEL_DIR']}")
    print(f"ä¸Šä¼ ç›®å½•: {app.config['UPLOAD_FOLDER']}")
    print(f"ç»“æœç›®å½•: {app.config['RESULTS_FOLDER']}")

    print(f"\nå¯åŠ¨WebæœåŠ¡å™¨ï¼Œç«¯å£: {port}")
    print(f"è¯·åœ¨æµè§ˆå™¨ä¸­è®¿é—®: http://localhost:{port}")
    print("====================================\n")

    # å¯åŠ¨åº”ç”¨
    app.run(host='0.0.0.0', port=port, debug=True)